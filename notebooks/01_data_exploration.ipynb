{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration: CICIDS2017 Dataset\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) on the CICIDS2017 intrusion detection dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and inspect the dataset\n",
    "2. Understand data structure and types\n",
    "3. Analyze label distribution\n",
    "4. Identify missing values and outliers\n",
    "5. Visualize feature distributions\n",
    "6. Explore correlations between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Figure size\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 3076: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load configuration\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../config/config.yaml\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     config = \u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[32m      6\u001b[39m RAW_DATA_PATH = config[\u001b[33m'\u001b[39m\u001b[33mpaths\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdata_raw\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\__init__.py:125\u001b[39m, in \u001b[36msafe_load\u001b[39m\u001b[34m(stream)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_load\u001b[39m(stream):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\__init__.py:79\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(stream, Loader)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(stream, Loader):\n\u001b[32m     75\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     loader = \u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m loader.get_single_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\loader.py:34\u001b[39m, in \u001b[36mSafeLoader.__init__\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mReader\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     Scanner.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     36\u001b[39m     Parser.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\reader.py:85\u001b[39m, in \u001b[36mReader.__init__\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.eof = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.raw_buffer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetermine_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\reader.py:124\u001b[39m, in \u001b[36mReader.determine_encoding\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eof \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.raw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw_buffer) < \u001b[32m2\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw_buffer, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw_buffer.startswith(codecs.BOM_UTF16_LE):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\yaml\\reader.py:178\u001b[39m, in \u001b[36mReader.update_raw\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, size=\u001b[32m4096\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m         \u001b[38;5;28mself\u001b[39m.raw_buffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x8f in position 3076: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_PATH = config['paths']['data_raw']\n",
    "LABEL_COLUMN = config['dataset']['label_column']\n",
    "\n",
    "print(f\"Raw data path: {RAW_DATA_PATH}\")\n",
    "print(f\"Label column: {LABEL_COLUMN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "We'll start by loading one file to explore, then load all files if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CSV files\n",
    "csv_files = list(Path(RAW_DATA_PATH).glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for idx, file in enumerate(csv_files, 1):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {idx}. {file.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first file for initial exploration\n",
    "if csv_files:\n",
    "    sample_file = csv_files[0]\n",
    "    print(f\"Loading: {sample_file.name}\")\n",
    "    df = pd.read_csv(sample_file, encoding='utf-8', low_memory=False)\n",
    "    print(f\"‚úì Loaded {len(df):,} records\")\n",
    "else:\n",
    "    print(\"‚úó No CSV files found. Please extract the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"  - Rows (samples): {df.shape[0]:,}\")\n",
    "print(f\"  - Columns (features): {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"\\n=== First 5 Rows ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "print(\"\\n=== Column Names ===\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "for idx, col in enumerate(df.columns, 1):\n",
    "    print(f\"{idx:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nColumn types:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"\\n=== Dataset Info ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Label distribution\n",
    "if LABEL_COLUMN in df.columns:\n",
    "    print(\"\\n=== Label Distribution ===\")\n",
    "    label_counts = df[LABEL_COLUMN].value_counts()\n",
    "    label_percentages = df[LABEL_COLUMN].value_counts(normalize=True) * 100\n",
    "    \n",
    "    label_df = pd.DataFrame({\n",
    "        'Count': label_counts,\n",
    "        'Percentage': label_percentages\n",
    "    })\n",
    "    \n",
    "    print(label_df)\n",
    "    print(f\"\\nTotal unique labels: {df[LABEL_COLUMN].nunique()}\")\n",
    "else:\n",
    "    print(f\"‚úó Label column '{LABEL_COLUMN}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Bar plot\n",
    "plt.subplot(1, 2, 1)\n",
    "label_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Attack Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "plt.subplot(1, 2, 2)\n",
    "label_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "print(f\"\\n‚ö† Class Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"  ‚Üí High imbalance detected! Consider using SMOTE or other sampling techniques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(\"\\n=== Missing Values Analysis ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nColumns with missing values: {len(missing_df)}\")\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_df['Percentage'].plot(kind='bar', color='coral')\n",
    "    plt.title('Missing Values Percentage by Column', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Column', fontsize=12)\n",
    "    plt.ylabel('Missing %', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úì No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Duplicate Records Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"\\n=== Duplicate Records ===\")\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicate_percentage = (duplicates / len(df)) * 100\n",
    "\n",
    "print(f\"Duplicate rows: {duplicates:,} ({duplicate_percentage:.2f}%)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"  ‚Üí Duplicates will be removed during preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\n=== Numerical Features ===\")\n",
    "print(f\"Total numerical features: {len(numerical_cols)}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"\\n=== Infinite Values ===\")\n",
    "inf_counts = {}\n",
    "for col in numerical_cols:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    print(f\"\\nColumns with infinite values: {len(inf_counts)}\")\n",
    "    for col, count in sorted(inf_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {col}: {count:,}\")\n",
    "else:\n",
    "    print(\"‚úì No infinite values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "print(\"\\n=== Feature Distributions ===\")\n",
    "\n",
    "# Select some key features to visualize\n",
    "key_features = [\n",
    "    'Flow Duration',\n",
    "    'Total Fwd Packets',\n",
    "    'Total Backward Packets',\n",
    "    'Flow Bytes/s',\n",
    "    'Flow Packets/s'\n",
    "]\n",
    "\n",
    "# Filter features that exist\n",
    "existing_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "if existing_features:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(existing_features[:6]):\n",
    "        # Remove infinite values for visualization\n",
    "        data = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        axes[idx].hist(data, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Value')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Remove extra subplots\n",
    "    for idx in range(len(existing_features), 6):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Distribution of Key Features', y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† Key features not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix (sample for performance)\n",
    "print(\"\\n=== Correlation Analysis ===\")\n",
    "print(\"Computing correlation matrix (this may take a moment)...\")\n",
    "\n",
    "# Use a sample for correlation analysis to speed up\n",
    "sample_size = min(10000, len(df))\n",
    "df_sample = df[numerical_cols].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Replace infinite values\n",
    "df_sample = df_sample.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "correlation_matrix = df_sample.corr()\n",
    "print(f\"‚úì Correlation matrix computed ({correlation_matrix.shape[0]}x{correlation_matrix.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix (top features)\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Select top 20 features with highest variance\n",
    "top_features = df_sample.var().nlargest(20).index\n",
    "corr_subset = correlation_matrix.loc[top_features, top_features]\n",
    "\n",
    "sns.heatmap(corr_subset, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix (Top 20 Features by Variance)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "print(\"\\n=== High Correlations (>0.95) ===\")\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.95:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nFound {len(high_corr_pairs)} highly correlated feature pairs:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "        print(f\"  {feat1[:40]:40s} <-> {feat2[:40]:40s} : {corr:.3f}\")\n",
    "    print(\"\\n  ‚Üí Consider removing one feature from each pair during feature selection\")\n",
    "else:\n",
    "    print(\"‚úì No highly correlated features found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Attack Types vs Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature distributions across attack types\n",
    "print(\"\\n=== Feature Analysis by Attack Type ===\")\n",
    "\n",
    "if LABEL_COLUMN in df.columns and existing_features:\n",
    "    # Select 2 key features for comparison\n",
    "    features_to_compare = existing_features[:2]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for idx, feature in enumerate(features_to_compare):\n",
    "        # Sample data for each label\n",
    "        plot_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for label in df[LABEL_COLUMN].unique()[:5]:  # Top 5 labels\n",
    "            data = df[df[LABEL_COLUMN] == label][feature].replace(\n",
    "                [np.inf, -np.inf], np.nan\n",
    "            ).dropna().sample(min(1000, len(df[df[LABEL_COLUMN] == label])))\n",
    "            plot_data.append(data)\n",
    "            labels.append(label)\n",
    "        \n",
    "        axes[idx].boxplot(plot_data, labels=labels)\n",
    "        axes[idx].set_title(f'{feature} by Attack Type', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Attack Type')\n",
    "        axes[idx].set_ylabel(feature)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Total features: {df.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  ‚Ä¢ Target classes: {df[LABEL_COLUMN].nunique()}\")\n",
    "\n",
    "print(f\"\\nüîç Data Quality:\")\n",
    "print(f\"  ‚Ä¢ Missing values: {df.isnull().sum().sum():,} ({(df.isnull().sum().sum()/df.size)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Duplicate rows: {duplicates:,} ({duplicate_percentage:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Columns with infinite values: {len(inf_counts)}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Class Imbalance:\")\n",
    "print(f\"  ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"  ‚Ä¢ Most common class: {label_counts.index[0]} ({label_counts.iloc[0]:,} samples)\")\n",
    "print(f\"  ‚Ä¢ Least common class: {label_counts.index[-1]} ({label_counts.iloc[-1]:,} samples)\")\n",
    "\n",
    "print(f\"\\nüîó Feature Correlation:\")\n",
    "print(f\"  ‚Ä¢ Highly correlated pairs (>0.95): {len(high_corr_pairs)}\")\n",
    "\n",
    "print(f\"\\nüìù Recommendations:\")\n",
    "print(f\"  1. ‚úì Remove duplicate rows during preprocessing\")\n",
    "print(f\"  2. ‚úì Handle missing values (drop or impute)\")\n",
    "print(f\"  3. ‚úì Replace infinite values with NaN\")\n",
    "print(f\"  4. ‚úì Apply SMOTE or other sampling for class imbalance\")\n",
    "print(f\"  5. ‚úì Remove highly correlated features\")\n",
    "print(f\"  6. ‚úì Normalize/standardize numerical features\")\n",
    "print(f\"  7. ‚úì Consider feature selection (PCA, feature importance)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Exploratory Data Analysis Complete!\")\n",
    "print(\"Next step: Run 02_data_preprocessing.ipynb\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optional: Load and Explore All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load ALL CSV files (warning: may take time and memory)\n",
    "\n",
    "# print(\"\\n=== Loading All CSV Files ===\")\n",
    "# dfs = []\n",
    "# \n",
    "# for file in csv_files:\n",
    "#     print(f\"Loading {file.name}...\")\n",
    "#     df_temp = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "#     dfs.append(df_temp)\n",
    "#     print(f\"  Loaded {len(df_temp):,} records\")\n",
    "# \n",
    "# df_all = pd.concat(dfs, ignore_index=True)\n",
    "# print(f\"\\n‚úì Total records across all files: {len(df_all):,}\")\n",
    "# \n",
    "# # Analyze combined dataset\n",
    "# print(\"\\n=== Combined Label Distribution ===\")\n",
    "# print(df_all[LABEL_COLUMN].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
