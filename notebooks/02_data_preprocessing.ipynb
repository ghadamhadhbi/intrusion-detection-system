{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing: CICIDS2017 Dataset\n",
    "\n",
    "This notebook handles data cleaning, transformation, and preparation for model training.\n",
    "\n",
    "## Objectives:\n",
    "1. Load raw dataset\n",
    "2. Clean data (remove duplicates, handle missing values)\n",
    "3. Handle infinite and outlier values\n",
    "4. Feature selection and engineering\n",
    "5. Label encoding\n",
    "6. Data normalization/standardization\n",
    "7. Train-test split\n",
    "8. Handle class imbalance\n",
    "9. Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Custom module\n",
    "from src.data_processing import DataProcessor\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_processing:Configuration loaded from ../config/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data processor initialized\n",
      "\n",
      "Configuration:\n",
      "  Raw data path: data/raw/\n",
      "  Processed data path: data/processed/\n",
      "  Normalization method: standard\n",
      "  Sampling method: SMOTE\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor\n",
    "processor = DataProcessor(config_path='../config/config.yaml')\n",
    "\n",
    "print(\"‚úì Data processor initialized\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Raw data path: {processor.config['paths']['data_raw']}\")\n",
    "print(f\"  Processed data path: {processor.config['paths']['data_processed']}\")\n",
    "print(f\"  Normalization method: {processor.config['preprocessing']['normalization']}\")\n",
    "print(f\"  Sampling method: {processor.config['preprocessing']['sampling']['method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "For this notebook, we'll use a sample of the data for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_processing:Loading dataset...\n",
      "WARNING:src.data_processing:File not found: data/raw/Monday-WorkingHours.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Wednesday-workingHours.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "WARNING:src.data_processing:File not found: data/raw/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10.0% of dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m SAMPLE_FRACTION = \u001b[32m0.1\u001b[39m  \u001b[38;5;66;03m# Use 10% of data\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLE_FRACTION*\u001b[32m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% of dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAMPLE_FRACTION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Dataset loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\notebooks\\..\\src\\data_processing.py:85\u001b[39m, in \u001b[36mDataProcessor.load_dataset\u001b[39m\u001b[34m(self, file_path, sample_frac)\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     83\u001b[39m             logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_full_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal records loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Sample data if needed (for testing)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    378\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:443\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    441\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    446\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghada\\intrusion-detection-system\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:505\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    502\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    508\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Load dataset (using 10% sample for notebook demonstration)\n",
    "SAMPLE_FRACTION = 0.1  # Use 10% of data\n",
    "\n",
    "print(f\"Loading {SAMPLE_FRACTION*100}% of dataset...\")\n",
    "df = processor.load_dataset(sample_frac=SAMPLE_FRACTION)\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Initial Data Quality ===\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {df.shape[1]}\")\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum():,}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum():,}\")\n",
    "\n",
    "# Check for infinite values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "inf_count = np.isinf(df[numeric_cols]).sum().sum()\n",
    "print(f\"Infinite values: {inf_count:,}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 1: Data Cleaning ===\")\n",
    "\n",
    "# Clean data\n",
    "df_clean = processor.clean_data(df)\n",
    "\n",
    "print(f\"\\n‚úì Cleaning complete\")\n",
    "print(f\"  Original rows: {len(df):,}\")\n",
    "print(f\"  After cleaning: {len(df_clean):,}\")\n",
    "print(f\"  Rows removed: {len(df) - len(df_clean):,} ({((len(df) - len(df_clean))/len(df)*100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning\n",
    "print(\"\\nPost-cleaning verification:\")\n",
    "print(f\"  Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 2: Feature Preparation ===\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X, y = processor.prepare_features(df_clean)\n",
    "\n",
    "print(f\"\\n‚úì Features prepared\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Label vector shape: {y.shape}\")\n",
    "print(f\"  Number of features: {X.shape[1]}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"\\nFeature names (first 10):\")\n",
    "for i, name in enumerate(feature_names[:10], 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution before encoding\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_dist = y.value_counts()\n",
    "print(label_dist)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "label_dist.plot(kind='bar', color='steelblue')\n",
    "plt.title('Label Distribution (Before Encoding)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 3: Label Encoding ===\")\n",
    "\n",
    "# Choose encoding type\n",
    "BINARY_CLASSIFICATION = False  # Set to True for binary, False for multi-class\n",
    "\n",
    "print(f\"Classification type: {'Binary' if BINARY_CLASSIFICATION else 'Multi-class'}\")\n",
    "\n",
    "# Encode labels\n",
    "y_encoded = processor.encode_labels(y, binary=BINARY_CLASSIFICATION)\n",
    "\n",
    "print(f\"\\n‚úì Labels encoded\")\n",
    "print(f\"  Unique labels: {len(np.unique(y_encoded))}\")\n",
    "print(f\"  Label range: [{y_encoded.min()}, {y_encoded.max()}]\")\n",
    "\n",
    "if not BINARY_CLASSIFICATION:\n",
    "    print(f\"\\nLabel mapping:\")\n",
    "    for idx, label in enumerate(processor.label_encoder.classes_):\n",
    "        print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoded label distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "plt.bar(unique, counts, color='coral')\n",
    "plt.title('Encoded Label Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Encoded Label')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nEncoded label distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = (count / len(y_encoded)) * 100\n",
    "    print(f\"  Label {label}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 4: Data Splitting ===\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data(X, y_encoded)\n",
    "\n",
    "print(f\"\\n‚úì Data split complete\")\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Training:   {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification\n",
    "print(\"\\nLabel distribution across splits:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nValidation set:\")\n",
    "print(pd.Series(y_val).value_counts().sort_index())\n",
    "print(f\"\\nTest set:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 5: Feature Normalization ===\")\n",
    "\n",
    "# Before normalization - show statistics\n",
    "print(\"\\nBefore normalization (training set):\")\n",
    "print(f\"  Mean: {X_train.mean().mean():.3f}\")\n",
    "print(f\"  Std:  {X_train.std().mean():.3f}\")\n",
    "print(f\"  Min:  {X_train.min().min():.3f}\")\n",
    "print(f\"  Max:  {X_train.max().max():.3f}\")\n",
    "\n",
    "# Normalize\n",
    "X_train_scaled, X_val_scaled = processor.normalize_features(X_train, X_val)\n",
    "_, X_test_scaled = processor.normalize_features(X_train, X_test)\n",
    "\n",
    "print(f\"\\n‚úì Normalization complete\")\n",
    "print(f\"  Method: {processor.config['preprocessing']['normalization']}\")\n",
    "\n",
    "# After normalization\n",
    "print(\"\\nAfter normalization (training set):\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.3f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.3f}\")\n",
    "print(f\"  Min:  {X_train_scaled.min():.3f}\")\n",
    "print(f\"  Max:  {X_train_scaled.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normalization effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before normalization\n",
    "sample_feature_idx = 0\n",
    "axes[0].hist(X_train.iloc[:, sample_feature_idx], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title(f'Before Normalization\\n{feature_names[sample_feature_idx]}', fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After normalization\n",
    "axes[1].hist(X_train_scaled[:, sample_feature_idx], bins=50, color='coral', edgecolor='black')\n",
    "axes[1].set_title(f'After Normalization\\n{feature_names[sample_feature_idx]}', fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 6: Handling Class Imbalance ===\")\n",
    "\n",
    "# Show imbalance before\n",
    "print(\"\\nClass distribution before resampling:\")\n",
    "unique_before, counts_before = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique_before, counts_before):\n",
    "    print(f\"  Label {label}: {count:,}\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = counts_before.max() / counts_before.min()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Apply resampling\n",
    "X_train_resampled, y_train_resampled = processor.handle_imbalance(X_train_scaled, y_train)\n",
    "\n",
    "# Show distribution after\n",
    "print(\"\\nClass distribution after resampling:\")\n",
    "unique_after, counts_after = np.unique(y_train_resampled, return_counts=True)\n",
    "for label, count in zip(unique_after, counts_after):\n",
    "    print(f\"  Label {label}: {count:,}\")\n",
    "\n",
    "print(f\"\\n‚úì Resampling complete\")\n",
    "print(f\"  Original training samples: {len(y_train):,}\")\n",
    "print(f\"  After resampling: {len(y_train_resampled):,}\")\n",
    "print(f\"  Samples added: {len(y_train_resampled) - len(y_train):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resampling effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before resampling\n",
    "axes[0].bar(unique_before, counts_before, color='skyblue')\n",
    "axes[0].set_title('Before Resampling', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Class Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# After resampling\n",
    "axes[1].bar(unique_after, counts_after, color='coral')\n",
    "axes[1].set_title(f'After Resampling ({processor.config[\"preprocessing\"][\"sampling\"][\"method\"]})', \n",
    "                  fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Class Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 7: Saving Processed Data ===\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "processed_path = processor.config['paths']['data_processed']\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "\n",
    "# Save all data\n",
    "processor.save_processed_data(\n",
    "    X_train_resampled, X_val_scaled, X_test_scaled,\n",
    "    y_train_resampled, y_val, y_test,\n",
    "    feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for file in Path(processed_path).glob('*'):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  ‚úì {file.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPROCESSING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Shapes:\")\n",
    "print(f\"  Training:   X={X_train_resampled.shape}, y={y_train_resampled.shape}\")\n",
    "print(f\"  Validation: X={X_val_scaled.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:       X={X_test_scaled.shape}, y={y_test.shape}\")\n",
    "\n",
    "print(f\"\\nüîß Preprocessing Steps Applied:\")\n",
    "print(f\"  ‚úì Data cleaning (duplicates, missing values, infinites)\")\n",
    "print(f\"  ‚úì Feature selection ({len(feature_names)} features retained)\")\n",
    "print(f\"  ‚úì Label encoding ({'Binary' if BINARY_CLASSIFICATION else 'Multi-class'})\")\n",
    "print(f\"  ‚úì Train-validation-test split\")\n",
    "print(f\"  ‚úì Feature normalization ({processor.config['preprocessing']['normalization']})\")\n",
    "print(f\"  ‚úì Class imbalance handling ({processor.config['preprocessing']['sampling']['method']})\")\n",
    "\n",
    "print(f\"\\nüìÅ Files Saved to: {processed_path}\")\n",
    "print(f\"  ‚úì X_train.npy, X_val.npy, X_test.npy\")\n",
    "print(f\"  ‚úì y_train.npy, y_val.npy, y_test.npy\")\n",
    "print(f\"  ‚úì scaler.pkl\")\n",
    "print(f\"  ‚úì label_encoder.pkl\")\n",
    "print(f\"  ‚úì feature_names.pkl\")\n",
    "\n",
    "print(f\"\\nüìà Class Distribution (Final Training Set):\")\n",
    "for label, count in zip(unique_after, counts_after):\n",
    "    percentage = (count / len(y_train_resampled)) * 100\n",
    "    if not BINARY_CLASSIFICATION and hasattr(processor.label_encoder, 'classes_'):\n",
    "        label_name = processor.label_encoder.classes_[label]\n",
    "        print(f\"  {label_name}: {count:,} ({percentage:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  Label {label}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(f\"\\nüìù Next Steps:\")\n",
    "print(f\"  1. Run 03_feature_engineering.ipynb for feature selection\")\n",
    "print(f\"  2. Run 04_model_training.ipynb to train ML models\")\n",
    "print(f\"  3. Run 05_model_evaluation.ipynb to evaluate performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Optional: Test Loading Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved data\n",
    "print(\"\\n=== Testing Data Loading ===\")\n",
    "\n",
    "# Load processed data\n",
    "X_train_loaded, X_val_loaded, X_test_loaded, y_train_loaded, y_val_loaded, y_test_loaded = processor.load_processed_data()\n",
    "\n",
    "print(\"\\n‚úì Data loaded successfully\")\n",
    "print(f\"\\nLoaded shapes:\")\n",
    "print(f\"  X_train: {X_train_loaded.shape}\")\n",
    "print(f\"  X_val: {X_val_loaded.shape}\")\n",
    "print(f\"  X_test: {X_test_loaded.shape}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nData integrity check:\")\n",
    "print(f\"  Training data match: {np.array_equal(X_train_resampled, X_train_loaded)}\")\n",
    "print(f\"  Validation data match: {np.array_equal(X_val_scaled, X_val_loaded)}\")\n",
    "print(f\"  Test data match: {np.array_equal(X_test_scaled, X_test_loaded)}\")\n",
    "\n",
    "# Load feature names\n",
    "loaded_features = joblib.load(os.path.join(processed_path, 'feature_names.pkl'))\n",
    "print(f\"\\n‚úì Feature names loaded: {len(loaded_features)} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
